# -*- coding: utf-8 -*-
"""MidSem_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f5_T7THC3xy4h_dsDw2XjlwakrUec5c0
"""

!pip install jsonpickle
!pip install twython

#Starting With Twitter

import tweepy                   # Python wrapper around Twitter API
from google.colab import drive  # to mount Drive to Colab notebook
from google.colab import files
import json
import csv
from datetime import date
from datetime import datetime
import time
from pprint import pprint
import jsonpickle

import numpy as np
import matplotlib.pyplot as plt
import re
from twython import Twython
from PIL import Image
from wordcloud import WordCloud, STOPWORDS
from IPython.display import Image as im

api_key_twitter = 'LcGhDfbafzUvzhku4lAamT9iK'
api_secret_key_twitter = 'V0PsntbwOhncDVkiaqDKcv1jQ4Xgf0b3HaLLA3vqZTRwE9arOf'

access_token_twitter = '1661809220-EstQSBAZm7uMmAZnNEDXtrjkNDrODnRw6YtyRd9'
access_token_secret_twitter = '6UjHcxXjgjDzcauqxWAm7eXKisScjJEjXub5rynoiOWDA'

auth = tweepy.OAuthHandler(api_key_twitter, api_secret_key_twitter)
auth.set_access_token(access_token_twitter, access_token_secret_twitter)
api = tweepy.API(auth)

query = 'gaurav arora'  # this is what we're searching for
en_lang = 'en' # this is used to specify the language of the tweets
popular_results = 'popular' # used to specifiy the order of tweet results. Accepted values: popular|recent|mixec
extended_mode = 'extended'
date_since = '2019-12-01'

search_results = tweepy.Cursor(api.search,
              q=query,
              lang=en_lang,
              tweet_mode = extended_mode).items(1000)


for tweet in search_results:
    pprint(tweet._json)

search_results = tweepy.Cursor(api.search,
              q=query,
              lang=en_lang,
              tweet_mode = extended_mode).items(1000)



file_name = 'twitter_data.json'
with open(file_name, 'w') as f:
    for tweet in search_results:
        pprint(tweet._json)
        f.write(jsonpickle.encode(tweet._json, unpicklable=False) +
                        '\n')


# Open the file in read mode
with open(file_name, 'r') as f:
    tweet_string_list = f.readlines()
    
# Convert Tweets from string to dict
tweet_list = []
for string in tweet_string_list:
    tweet_list.append(json.loads(string))

files.download('twitter_data.json')

raw_tweets = []
for tweets in tweet_list:
      # raw_tweets.append(tweets['user']['name'])
  name = tweets['user']['name']
  username = tweets['user']['screen_name']
  about = tweets['user']['description']
  fR = tweets['user']['followers_count']
  fG = tweets['user']['friends_count']
  loc = tweets['user']['location']
  post = tweets['user']['statuses_count']

print(name)
print('@',username, sep='')
print(about)
print(fG)
print(fR)
print(loc)
print(post)

raw_tweets

raw_string = ''.join(raw_tweets)
no_links = re.sub(r'http\S+', '', raw_string)
no_unicode = re.sub(r"\\[a-z][a-z]?[0-9]+", '', no_links)
no_special_characters = re.sub('[^A-Za-z ]+', '', no_unicode)

raw_string

words = no_special_characters.split(" ")
words = [w for w in words if len(w) > 2]  # ignore a, an, be, ...
words = [w.lower() for w in words]
words = [w for w in words if w not in STOPWORDS]

words

